[
["index.html", "Intro to Data and Probability Summary", " Intro to Data and Probability Akshay Kotha 2018-12-30 Summary Check out the course activity week-by-week by navigation help from left. Github repository link for this course: https://github.com/akshayreddykotha/intro-data-prob "],
["week-1-introduction-to-r-and-rstudio.html", "Week 1 - Introduction to R and RStudio 0.1 RStudio 0.2 R Packages 0.3 Dataset 1: Dr. Arbuthnot’s Baptism Records 0.4 Dataset 2: Present birth records 0.5 Resources for learning R and working in RStudio", " Week 1 - Introduction to R and RStudio Complete all Exercises, and submit answers to Questions on the Coursera platform. The goal of this lab is to introduce you to R and RStudio, which you’ll be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions. To straighten out which is which: R is the name of the programming language itself and RStudio is a convenient interface. As the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands. 0.1 RStudio Your RStudio window has four panels. Your R Markdown file (this document) is in the upper left panel. The panel on the lower left is where the action happens. It’s called the console. Everytime you launch RStudio, it will have the same text at the top of the console telling you the version of R that you’re running. Below that information is the prompt. As its name suggests, this prompt is really a request, a request for a command. Initially, interacting with R is all about typing commands and interpreting the output. These commands and their syntax have evolved over decades (literally) and now provide what many users feel is a fairly natural way to access data and organize, describe, and invoke statistical computations. The panel in the upper right contains your workspace as well as a history of the commands that you’ve previously entered. Any plots that you generate will show up in the panel in the lower right corner. This is also where you can browse your files, access help, manage packages, etc. 0.2 R Packages R is an open-source programming language, meaning that users can contribute packages that make our lives easier, and we can use them for free. For this lab, and many others in the future, we will use the following R packages: statsr: for data files and functions used in this course dplyr: for data wrangling ggplot2: for data visualization You should have already installed these packages using commands like install.packages and install_github. Next, you need to load the packages in your working environment. We do this with the library function. Note that you only need to install packages once, but you need to load them each time you relaunch RStudio. library(dplyr) library(ggplot2) library(statsr) To do so, you can click on the green arrow at the top of the code chunk in the R Markdown (Rmd) file, or highlight these lines, and hit the Run button on the upper right corner of the pane, or type the code in the console. Going forward you will be asked to load any relevant packages at the beginning of each lab. 0.3 Dataset 1: Dr. Arbuthnot’s Baptism Records To get you started, run the following command to load the data. data(arbuthnot) To do so, once again, you can click on the green arrow at the top of the code chunk in the R Markdown (Rmd) file, or put your cursor on this line, and hit the Run button on the upper right corner of the pane, or type the code in the console. This command instructs R to load some data. The Arbuthnot baptism counts for boys and girls. You should see that the workspace area in the upper righthand corner of the RStudio window now lists a data set called arbuthnot that has 82 observations on 3 variables. As you interact with R, you will create a series of objects. Sometimes you load them as we have done here, and sometimes you create them yourself as the byproduct of a computation or some analysis you have performed. The Arbuthnot data set refers to Dr. John Arbuthnot, an 18th century physician, writer, and mathematician. He was interested in the ratio of newborn boys to newborn girls, so he gathered the baptism records for children born in London for every year from 1629 to 1710. We can take a look at the data by typing its name into the console. arbuthnot ## # A tibble: 82 x 3 ## year boys girls ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1629 5218 4683 ## 2 1630 4858 4457 ## 3 1631 4422 4102 ## 4 1632 4994 4590 ## 5 1633 5158 4839 ## 6 1634 5035 4820 ## 7 1635 5106 4928 ## 8 1636 4917 4605 ## 9 1637 4703 4457 ## 10 1638 5359 4952 ## # ... with 72 more rows However printing the whole dataset in the console is not that useful. One advantage of RStudio is that it comes with a built-in data viewer. Click on the name arbuthnot in the Environment pane (upper right window) that lists the objects in your workspace. This will bring up an alternative display of the data set in the Data Viewer (upper left window). You can close the data viewer by clicking on the x in the upper lefthand corner. What you should see are four columns of numbers, each row representing a different year: the first entry in each row is simply the row number (an index we can use to access the data from individual years if we want), the second is the year, and the third and fourth are the numbers of boys and girls baptized that year, respectively. Use the scrollbar on the right side of the console window to examine the complete data set. Note that the row numbers in the first column are not part of Arbuthnot’s data. R adds them as part of its printout to help you make visual comparisons. You can think of them as the index that you see on the left side of a spreadsheet. In fact, the comparison to a spreadsheet will generally be helpful. R has stored Arbuthnot’s data in a kind of spreadsheet or table called a data frame. You can see the dimensions of this data frame by typing: dim(arbuthnot) ## [1] 82 3 This command should output [1] 82 3, indicating that there are 82 rows and 3 columns (we’ll get to what the [1] means in a bit), just as it says next to the object in your workspace. You can see the names of these columns (or variables) by typing: names(arbuthnot) ## [1] &quot;year&quot; &quot;boys&quot; &quot;girls&quot; How many variables are included in this data set? 2 3 4 82 1710 Exercise: What years are included in this dataset? Hint: Take a look at the year variable in the Data Viewer to answer this question. You should see that the data frame contains the columns year, boys, and girls. At this point, you might notice that many of the commands in R look a lot like functions from math class; that is, invoking R commands means supplying a function with some number of arguments. The dim and names commands, for example, each took a single argument, the name of a data frame. Tip: If you use the up and down arrow keys, you can scroll through your previous commands, your so-called command history. You can also access it by clicking on the history tab in the upper right panel. This will save you a lot of typing in the future. 0.3.1 R Markdown So far we asked you to type your commands in the console. The console is a great place for playing around with some code, however it is not a good place for documenting your work. Working in the console exclusively makes it difficult to document your work as you go, and reproduce it later. R Markdown is a great solution for this problem. And, you already have worked with an R Markdown document – this lab! Going forward type the code for the questions in the code chunks provided in the R Markdown (Rmd) document for the lab, and Knit the document to see the results. 0.3.2 Some Exploration Let’s start to examine the data a little more closely. We can access the data in a single column of a data frame separately using a command like arbuthnot$boys ## [1] 5218 4858 4422 4994 5158 5035 5106 4917 4703 5359 5366 5518 5470 5460 ## [15] 4793 4107 4047 3768 3796 3363 3079 2890 3231 3220 3196 3441 3655 3668 ## [29] 3396 3157 3209 3724 4748 5216 5411 6041 5114 4678 5616 6073 6506 6278 ## [43] 6449 6443 6073 6113 6058 6552 6423 6568 6247 6548 6822 6909 7577 7575 ## [57] 7484 7575 7737 7487 7604 7909 7662 7602 7676 6985 7263 7632 8062 8426 ## [71] 7911 7578 8102 8031 7765 6113 8366 7952 8379 8239 7840 7640 This command will only show the number of boys baptized each year. The dollar sign basically says “go to the data frame that comes before me, and find the variable that comes after me”. What command would you use to extract just the counts of girls born? arbuthnot$boys arbuthnot$girls girls arbuthnot[girls] $girls # type your code for the Question 2 here, and Knit arbuthnot$girls ## [1] 4683 4457 4102 4590 4839 4820 4928 4605 4457 4952 4784 5332 5200 4910 ## [15] 4617 3997 3919 3395 3536 3181 2746 2722 2840 2908 2959 3179 3349 3382 ## [29] 3289 3013 2781 3247 4107 4803 4881 5681 4858 4319 5322 5560 5829 5719 ## [43] 6061 6120 5822 5738 5717 5847 6203 6033 6041 6299 6533 6744 7158 7127 ## [57] 7246 7119 7214 7101 7167 7302 7392 7316 7483 6647 6713 7229 7767 7626 ## [71] 7452 7061 7514 7656 7683 5738 7779 7417 7687 7623 7380 7288 Notice that the way R has printed these data is different. When we looked at the complete data frame, we saw 82 rows, one on each line of the display. These data are no longer structured in a table with other variables, so they are displayed one right after another. Objects that print out in this way are called vectors; they represent a set of numbers. R has added numbers in [brackets] along the left side of the printout to indicate locations within the vector. For example, in the arbuthnot$boys vector, 5218 follows [1], indicating that 5218 is the first entry in the vector. And if [43] starts a line, then that would mean the first number on that line would represent the 43rd entry in the vector. R has some powerful functions for making graphics. We can create a simple plot of the number of girls baptized per year with the command ggplot(data = arbuthnot, aes(x = year, y = girls)) + geom_point() Before we review the code for this plot, let’s summarize the trends we see in the data. Which of the following best describes the number of girls baptised over the years included in this dataset? There appears to be no trend in the number of girls baptised from 1629 to 1710. There is initially an increase in the number of girls baptised, which peaks around 1640. After 1640 there is a decrease in the number of girls baptised, but the number begins to increase again in 1660. Overall the trend is an increase in the number of girls baptised. There is initially an increase in the number of girls baptised. This number peaks around 1640 and then after 1640 the number of girls baptised decreases. The number of girls baptised has decreased over time. There is an initial increase in the number of girls baptised but this number appears to level around 1680 and not change after that time point. Back to the code… We use the ggplot() function to build plots. If you run the plotting code in your console, you should see the plot appear under the Plots tab of the lower right panel of RStudio. Notice that the command above again looks like a function, this time with arguments separated by commas. The first argument is always the dataset. Next, we provide thevariables from the dataset to be assigned to aesthetic elements of the plot, e.g. the x and the y axes. Finally, we use another layer, separated by a + to specify the geometric object for the plot. Since we want to scatterplot, we use geom_point. You might wonder how you are supposed to know the syntax for the ggplot function. Thankfully, R documents all of its functions extensively. To read what a function does and learn the arguments that are available to you, just type in a question mark followed by the name of the function that you’re interested in. Try the following in your console: #?ggplot Notice that the help file replaces the plot in the lower right panel. You can toggle between plots and help files using the tabs at the top of that panel. More extensive help for plotting with the ggplot2 package can be found at http://docs.ggplot2.org/current/. The best (and easiest) way to learn the syntax is to take a look at the sample plots provided on that page, and modify the code bit by bit until you get achieve the plot you want. 0.3.3 R as a big calculator Now, suppose we want to plot the total number of baptisms. To compute this, we could use the fact that R is really just a big calculator. We can type in mathematical expressions like 5218 + 4683 ## [1] 9901 to see the total number of baptisms in 1629. We could repeat this once for each year, but there is a faster way. If we add the vector for baptisms for boys to that of girls, R will compute all sums simultaneously. arbuthnot$boys + arbuthnot$girls ## [1] 9901 9315 8524 9584 9997 9855 10034 9522 9160 10311 10150 ## [12] 10850 10670 10370 9410 8104 7966 7163 7332 6544 5825 5612 ## [23] 6071 6128 6155 6620 7004 7050 6685 6170 5990 6971 8855 ## [34] 10019 10292 11722 9972 8997 10938 11633 12335 11997 12510 12563 ## [45] 11895 11851 11775 12399 12626 12601 12288 12847 13355 13653 14735 ## [56] 14702 14730 14694 14951 14588 14771 15211 15054 14918 15159 13632 ## [67] 13976 14861 15829 16052 15363 14639 15616 15687 15448 11851 16145 ## [78] 15369 16066 15862 15220 14928 What you will see are 82 numbers (in that packed display, because we aren’t looking at a data frame here), each one representing the sum we’re after. Take a look at a few of them and verify that they are right. 0.3.4 Adding a new variable to the data frame We’ll be using this new vector to generate some plots, so we’ll want to save it as a permanent column in our data frame. arbuthnot &lt;- arbuthnot %&gt;% mutate(total = boys + girls) What in the world is going on here? The %&gt;% operator is called the piping operator. Basically, it takes the output of the current line and pipes it into the following line of code. A note on piping: Note that we can read these three lines of code as the following: “Take the arbuthnot dataset and pipe it into the mutate function. Using this mutate a new variable called total that is the sum of the variables called boys and girls. Then assign this new resulting dataset to the object called arbuthnot, i.e. overwrite the old arbuthnot dataset with the new one containing the new variable.” This is essentially equivalent to going through each row and adding up the boys and girls counts for that year and recording that value in a new column called total. Where is the new variable? When you make changes to variables in your dataset, click on the name of the dataset again to update it in the data viewer. You’ll see that there is now a new column called total that has been tacked on to the data frame. The special symbol &lt;- performs an assignment, taking the output of one line of code and saving it into an object in your workspace. In this case, you already have an object called arbuthnot, so this command updates that data set with the new mutated column. We can make a plot of the total number of baptisms per year with the following command. ggplot(data = arbuthnot, aes(x = year, y = total)) + geom_line() Note that using geom_line() instead of geom_point() results in a line plot instead of a scatter plot. You want both? Just layer them on: ggplot(data = arbuthnot, aes(x = year, y = total)) + geom_line() + geom_point() Exercise: Now, generate a plot of the proportion of boys born over time. What do you see? ggplot(data = arbuthnot, aes(x = year, y = boys/total)) + geom_line() + geom_point() Finally, in addition to simple mathematical operators like subtraction and division, you can ask R to make comparisons like greater than, &gt;, less than, &lt;, and equality, ==. For example, we can ask if boys outnumber girls in each year with the expression arbuthnot &lt;- arbuthnot %&gt;% mutate(more_boys = boys &gt; girls) This command add a new variable to the arbuthnot data frame containing the values of either TRUE if that year had more boys than girls, or FALSE if that year did not (the answer may surprise you). This variable contains different kind of data than we have considered so far. All other columns in the arbuthnot data frame have values are numerical (the year, the number of boys and girls). Here, we’ve asked R to create logical data, data where the values are either TRUE or FALSE. In general, data analysis will involve many different kinds of data types, and one reason for using R is that it is able to represent and compute with many of them. 0.4 Dataset 2: Present birth records In the previous few pages, you recreated some of the displays and preliminary analysis of Arbuthnot’s baptism data. Next you will do a similar analysis, but for present day birth records in the United States. Load up the present day data with the following command. data(present) The data are stored in a data frame called present which should now be loaded in your workspace. How many variables are included in this data set? 2 3 4 74 2013 dim(present) ## [1] 74 3 Exercise: What years are included in this dataset? Hint: Use the range function and present$year as its argument. range(present$year) ## [1] 1940 2013 Calculate the total number of births for each year and store these values in a new variable called total in the present dataset. Then, calculate the proportion of boys born each year and store these values in a new variable called prop_boys in the same dataset. Plot these values over time and based on the plot determine if the following statement is true or false: The proportion of boys born in the US has decreased over time. True False # type your code for Question 5 here, and Knit present &lt;- present %&gt;% mutate(total = boys + girls) present &lt;- present %&gt;% mutate(prop_boys = (boys/total)) ggplot(data = present, aes(x = year, y = prop_boys)) + geom_line() Create a new variable called more_boys which contains the value of either TRUE if that year had more boys than girls, or FALSE if that year did not. Based on this variable which of the following statements is true? Every year there are more girls born than boys. Every year there are more boys born than girls. Half of the years there are more boys born, and the other half more girls born. # type your code for Question 6 here, and Knit present &lt;- present %&gt;% mutate(more_boys = boys &gt; girls) Calculate the boy-to-girl ratio each year, and store these values in a new variable called prop_boy_girl in the present dataset. Plot these values over time. Which of the following best describes the trend? There appears to be no trend in the boy-to-girl ratio from 1940 to 2013. There is initially an increase in boy-to-girl ratio, which peaks around 1960. After 1960 there is a decrease in the boy-to-girl ratio, but the number begins to increase in the mid 1970s. There is initially a decrease in the boy-to-girl ratio, and then an increase between 1960 and 1970, followed by a decrease. The boy-to-girl ratio has increased over time. There is an initial decrease in the boy-to-girl ratio born but this number appears to level around 1960 and remain constant since then. # type your code for Question 7 here, and Knit present &lt;- present %&gt;% mutate(prop_boy_girl = boys/girls) ggplot(data = present, aes (x = year, y = prop_boy_girl)) + geom_point() In what year did we see the most total number of births in the U.S.? Hint: Sort your dataset in descending order based on the total column. You can do this interactively in the data viewer by clicking on the arrows next to the variable names. Or to arrange the data in a descenting order with new function: descr (for descending order). 1940 1957 1961 1991 2007 # type your code for Question 8 here # sample code is provided below, edit as necessary, uncomment, and then Knit present %&gt;% mutate(total = boys +girls) %&gt;% arrange(desc(total)) ## # A tibble: 74 x 7 ## year boys girls total prop_boys more_boys prop_boy_girl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 2007 2208071 2108162 4316233 0.512 TRUE 1.05 ## 2 1961 2186274 2082052 4268326 0.512 TRUE 1.05 ## 3 2006 2184237 2081318 4265555 0.512 TRUE 1.05 ## 4 1960 2179708 2078142 4257850 0.512 TRUE 1.05 ## 5 1957 2179960 2074824 4254784 0.512 TRUE 1.05 ## 6 2008 2173625 2074069 4247694 0.512 TRUE 1.05 ## 7 1959 2173638 2071158 4244796 0.512 TRUE 1.05 ## 8 1958 2152546 2051266 4203812 0.512 TRUE 1.05 ## 9 1962 2132466 2034896 4167362 0.512 TRUE 1.05 ## 10 1956 2133588 2029502 4163090 0.513 TRUE 1.05 ## # ... with 64 more rows 0.5 Resources for learning R and working in RStudio That was a short introduction to R and RStudio, but we will provide you with more functions and a more complete sense of the language as the course progresses. You might find the following tips and resources helpful. In this course we will be using the dplyr (for data wrangling) and ggplot2 (for data visualization) extensively. If you are googling for R code, make sure to also include these package names in your search query. For example, instead of googling “scatterplot in R”, google “scatterplot in R with ggplot2”. The following cheathseets may come in handy throughout the course. Note that some of the code on these cheatsheets may be too advanced for this course, however majority of it will become useful as you progress through the course material. Data wrangling cheatsheet Data visualization cheatsheet R Markdown While you will get plenty of exercise working with these packages in the labs of this course, if you would like further opportunities to practice we recommend checking out the relevant courses at DataCamp. This is a derivative of an OpenIntro lab, and is released under a Attribution-NonCommercial-ShareAlike 3.0 United States license. "],
["week-2-introduction-to-data.html", "Week 2 - Introduction to data 0.6 Getting started 0.7 Analysis", " Week 2 - Introduction to data Complete all Exercises, and submit answers to Questions on the Coursera platform. Some define statistics as the field that focuses on turning information into knowledge. The first step in that process is to summarize and describe the raw information - the data. In this lab we explore flights, specifically a random sample of domestic flights that departed from the three major New York City airport in 2013. We will generate simple graphical and numerical summaries of data on these flights and explore delay times. As this is a large data set, along the way you’ll also learn the indispensable skills of data processing and subsetting. 0.6 Getting started 0.6.1 Load packages In this lab we will explore the data using the dplyr package and visualize it using the ggplot2 package for data visualization. The data can be found in the companion package for this course, statsr. Let’s load the packages. library(statsr) library(dplyr) library(ggplot2) 0.6.2 Data The Bureau of Transportation Statistics (BTS) is a statistical agency that is a part of the Research and Innovative Technology Administration (RITA). As its name implies, BTS collects and makes available transportation data, such as the flights data we will be working with in this lab. We begin by loading the nycflights data frame. Type the following in your console to load the data: data(nycflights) The data frame containing 32735 flights that shows up in your workspace is a data matrix, with each row representing an observation and each column representing a variable. R calls this data format a data frame, which is a term that will be used throughout the labs. To view the names of the variables, type the command names(nycflights) ## [1] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;dep_time&quot; &quot;dep_delay&quot; ## [6] &quot;arr_time&quot; &quot;arr_delay&quot; &quot;carrier&quot; &quot;tailnum&quot; &quot;flight&quot; ## [11] &quot;origin&quot; &quot;dest&quot; &quot;air_time&quot; &quot;distance&quot; &quot;hour&quot; ## [16] &quot;minute&quot; This returns the names of the variables in this data frame. The codebook (description of the variables) is included below. This information can also be found in the help file for the data frame which can be accessed by typing ?nycflights in the console. year, month, day: Date of departure dep_time, arr_time: Departure and arrival times, local timezone. dep_delay, arr_delay: Departure and arrival delays, in minutes. Negative times represent early departures/arrivals. carrier: Two letter carrier abbreviation. 9E: Endeavor Air Inc. AA: American Airlines Inc. AS: Alaska Airlines Inc. B6: JetBlue Airways DL: Delta Air Lines Inc. EV: ExpressJet Airlines Inc. F9: Frontier Airlines Inc. FL: AirTran Airways Corporation HA: Hawaiian Airlines Inc. MQ: Envoy Air OO: SkyWest Airlines Inc. UA: United Air Lines Inc. US: US Airways Inc. VX: Virgin America WN: Southwest Airlines Co. YV: Mesa Airlines Inc. tailnum: Plane tail number flight: Flight number origin, dest: Airport codes for origin and destination. (Google can help you with what code stands for which airport.) air_time: Amount of time spent in the air, in minutes. distance: Distance flown, in miles. hour, minute: Time of departure broken in to hour and minutes. A very useful function for taking a quick peek at your data frame, and viewing its dimensions and data types is str, which stands for structure. str(nycflights) ## Classes &#39;tbl_df&#39; and &#39;data.frame&#39;: 32735 obs. of 16 variables: ## $ year : int 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ month : int 6 5 12 5 7 1 12 8 9 4 ... ## $ day : int 30 7 8 14 21 1 9 13 26 30 ... ## $ dep_time : int 940 1657 859 1841 1102 1817 1259 1920 725 1323 ... ## $ dep_delay: num 15 -3 -1 -4 -3 -3 14 85 -10 62 ... ## $ arr_time : int 1216 2104 1238 2122 1230 2008 1617 2032 1027 1549 ... ## $ arr_delay: num -4 10 11 -34 -8 3 22 71 -8 60 ... ## $ carrier : chr &quot;VX&quot; &quot;DL&quot; &quot;DL&quot; &quot;DL&quot; ... ## $ tailnum : chr &quot;N626VA&quot; &quot;N3760C&quot; &quot;N712TW&quot; &quot;N914DL&quot; ... ## $ flight : int 407 329 422 2391 3652 353 1428 1407 2279 4162 ... ## $ origin : chr &quot;JFK&quot; &quot;JFK&quot; &quot;JFK&quot; &quot;JFK&quot; ... ## $ dest : chr &quot;LAX&quot; &quot;SJU&quot; &quot;LAX&quot; &quot;TPA&quot; ... ## $ air_time : num 313 216 376 135 50 138 240 48 148 110 ... ## $ distance : num 2475 1598 2475 1005 296 ... ## $ hour : num 9 16 8 18 11 18 12 19 7 13 ... ## $ minute : num 40 57 59 41 2 17 59 20 25 23 ... The nycflights data frame is a massive trove of information. Let’s think about some questions we might want to answer with these data: We might want to find out how delayed flights headed to a particular destination tend to be. We might want to evaluate how departure delays vary over months. Or we might want to determine which of the three major NYC airports has a better on time percentage for departing flights. 0.6.3 Seven verbs The dplyr package offers seven verbs (functions) for basic data manipulation: filter() arrange() select() distinct() mutate() summarise() sample_n() We will use some of these functions in this lab, and learn about others in a future lab. 0.7 Analysis 0.7.1 Departure delays in flights to Raleigh-Durham (RDU) We can examine the distribution of departure delays of all flights with a histogram. ggplot(data = nycflights, aes(x = dep_delay)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This function says to plot the dep_delay variable from the nycflights data frame on the x-axis. It also defines a geom (short for geometric object), which describes the type of plot you will produce. Histograms are generally a very good way to see the shape of a single distribution, but that shape can change depending on how the data is split between the different bins. You can easily define the binwidth you want to use: ggplot(data = nycflights, aes(x = dep_delay)) + geom_histogram(binwidth = 15) ggplot(data = nycflights, aes(x = dep_delay)) + geom_histogram(binwidth = 150) Exercise: How do these three histograms with the various binwidths compare? If we want to focus on departure delays of flights headed to RDU only, we need to first filter the data for flights headed to RDU (dest == &quot;RDU&quot;) and then make a histogram of only departure delays of only those flights. rdu_flights &lt;- nycflights %&gt;% filter(dest == &quot;RDU&quot;) ggplot(data = rdu_flights, aes(x = dep_delay)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Let’s decipher these three lines of code: Line 1: Take the nycflights data frame, filter for flights headed to RDU, and save the result as a new data frame called rdu_flights. == means “if it’s equal to”. RDU is in quotation marks since it is a character string. Line 2: Basically the same ggplot call from earlier for making a histogram, except that it uses the data frame for flights headed to RDU instead of all flights. Logical operators: Filtering for certain observations (e.g. flights from a particular airport) is often of interest in data frames where we might want to examine observations with certain characteristics separately from the rest of the data. To do so we use the filter function and a series of logical operators. The most commonly used logical operators for data analysis are as follows: == means “equal to” != means “not equal to” &gt; or &lt; means “greater than” or “less than” &gt;= or &lt;= means “greater than or equal to” or “less than or equal to” We can also obtain numerical summaries for these flights: rdu_flights %&gt;% summarise(mean_dd = mean(dep_delay), sd_dd = sd(dep_delay), n = n()) ## # A tibble: 1 x 3 ## mean_dd sd_dd n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 11.7 35.6 801 Note that in the summarise function we created a list of two elements. The names of these elements are user defined, like mean_dd, sd_dd, n, and you could customize these names as you like (just don’t use spaces in your names). Calculating these summary statistics also require that you know the function calls. Note that n() reports the sample size. Summary statistics: Some useful function calls for summary statistics for a single numerical variable are as follows: mean median sd var IQR range min max We can also filter based on multiple criteria. Suppose we are interested in flights headed to San Francisco (SFO) in February: sfo_feb_flights &lt;- nycflights %&gt;% filter(dest == &quot;SFO&quot;, month == 2) Note that we can separate the conditions using commas if we want flights that are both headed to SFO and in February. If we are interested in either flights headed to SFO or in February we can use the | instead of the comma. Create a new data frame that includes flights headed to SFO in February, and save this data frame as sfo_feb_flights. How many flights meet these criteria? 68 1345 2286 3563 32735 # type your code for Question 1 here, and Knit sfo_feb_flights &lt;- nycflights %&gt;% filter(dest == &quot;SFO&quot;, month == 2) sfo_feb_flights %&gt;% summarise(n = n()) ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 68 Make a histogram and calculate appropriate summary statistics for arrival delays of sfo_feb_flights. Which of the following is false? The distribution is unimodal. The distribution is right skewed. No flight is delayed more than 2 hours. The distribution has several extreme values on the right side. More than 50% of flights arrive on time or earlier than scheduled. # type your code for Question 2 here, and Knit # ggplot(data = sfo_feb_flights, aes(x = arr_delay)) + geom_histogram(binwidth = 10) sfo_feb_flights %&gt;% summarise(sd_ad = sd(arr_delay), var_ad = var(arr_delay), iqr_ad = IQR(arr_delay), median_ad = median(arr_delay), min_ad = min(arr_delay), max_ad = max(arr_delay)) ## # A tibble: 1 x 6 ## sd_ad var_ad iqr_ad median_ad min_ad max_ad ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 36.3 1316. 23.2 -11 -66 196 sfo_feb_flights_ear &lt;- sfo_feb_flights %&gt;% filter(arr_delay &lt;= 0) sfo_feb_flights_ear %&gt;% summarise(n = n()) ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 49 Another useful functionality is being able to quickly calculate summary statistics for various groups in your data frame. For example, we can modify the above command using the group_by function to get the same summary stats for each origin airport: rdu_flights %&gt;% group_by(origin) %&gt;% summarise(mean_dd = mean(dep_delay), sd_dd = sd(dep_delay), n = n()) ## # A tibble: 3 x 4 ## origin mean_dd sd_dd n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 EWR 13.4 32.1 145 ## 2 JFK 15.4 40.3 300 ## 3 LGA 7.90 32.2 356 Here, we first grouped the data by origin, and then calculated the summary statistics. Calculate the median and interquartile range for arr_delays of flights in the sfo_feb_flights data frame, grouped by carrier. Which carrier has the hights IQR of arrival delays? American Airlines JetBlue Airways Virgin America Delta and United Airlines Frontier Airlines # type your code for Question 3 here, and Knit sfo_feb_flights %&gt;% group_by(carrier) %&gt;% summarise(median_ad = median(arr_delay), iqr_ad = IQR(arr_delay), n = n()) %&gt;% arrange(desc(iqr_ad)) ## # A tibble: 5 x 4 ## carrier median_ad iqr_ad n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 DL -15 22 19 ## 2 UA -10 22 21 ## 3 VX -22.5 21.2 12 ## 4 AA 5 17.5 10 ## 5 B6 -10.5 12.2 6 0.7.2 Departure delays over months Which month would you expect to have the highest average delay departing from an NYC airport? Let’s think about how we would answer this question: First, calculate monthly averages for departure delays. With the new language we are learning, we need to group_by months, then summarise mean departure delays. Then, we need to arrange these average delays in descending order nycflights %&gt;% group_by(month) %&gt;% summarise(mean_dd = mean(dep_delay)) %&gt;% arrange(desc(mean_dd)) ## # A tibble: 12 x 2 ## month mean_dd ## &lt;int&gt; &lt;dbl&gt; ## 1 7 20.8 ## 2 6 20.4 ## 3 12 17.4 ## 4 4 14.6 ## 5 3 13.5 ## 6 5 13.3 ## 7 8 12.6 ## 8 2 10.7 ## 9 1 10.2 ## 10 9 6.87 ## 11 11 6.10 ## 12 10 5.88 Which month has the highest average departure delay from an NYC airport? January March July October December # type your code for Question 4 here, and Knit nycflights %&gt;% group_by(month) %&gt;% summarise(mean_dd = mean(dep_delay)) %&gt;% arrange(desc(mean_dd)) ## # A tibble: 12 x 2 ## month mean_dd ## &lt;int&gt; &lt;dbl&gt; ## 1 7 20.8 ## 2 6 20.4 ## 3 12 17.4 ## 4 4 14.6 ## 5 3 13.5 ## 6 5 13.3 ## 7 8 12.6 ## 8 2 10.7 ## 9 1 10.2 ## 10 9 6.87 ## 11 11 6.10 ## 12 10 5.88 Which month has the highest median departure delay from an NYC airport? January March July October December # type your code for Question 5 here, and Knit nycflights %&gt;% group_by(month) %&gt;% summarise(median_dd = median(dep_delay)) %&gt;% arrange(desc(median_dd)) ## # A tibble: 12 x 2 ## month median_dd ## &lt;int&gt; &lt;dbl&gt; ## 1 12 1 ## 2 6 0 ## 3 7 0 ## 4 3 -1 ## 5 5 -1 ## 6 8 -1 ## 7 1 -2 ## 8 2 -2 ## 9 4 -2 ## 10 11 -2 ## 11 9 -3 ## 12 10 -3 Is the mean or the median a more reliable measure for deciding which month(s) to avoid flying if you really dislike delayed flights, and why? Mean would be more reliable as it gives us the true average. Mean would be more reliable as the distribution of delays is symmetric. Median would be more reliable as the distribution of delays is skewed. Median would be more reliable as the distribution of delays is symmetric. Both give us useful information. We can also visualize the distributions of departure delays across months using side-by-side box plots: ggplot(nycflights, aes(x = factor(month), y = dep_delay)) + geom_boxplot() There is some new syntax here: We want departure delays on the y-axis and the months on the x-axis to produce side-by-side box plots. Side-by-side box plots require a categorical variable on the x-axis, however in the data frame month is stored as a numerical variable (numbers 1 - 12). Therefore we can force R to treat this variable as categorical, what R calls a factor, variable with factor(month). 0.7.3 On time departure rate for NYC airports Suppose you will be flying out of NYC and want to know which of the three major NYC airports has the best on time departure rate of departing flights. Suppose also that for you a flight that is delayed for less than 5 minutes is basically “on time”. You consider any flight delayed for 5 minutes of more to be “delayed”. In order to determine which airport has the best on time departure rate, we need to first classify each flight as “on time” or “delayed”, then group flights by origin airport, then calculate on time departure rates for each origin airport, and finally arrange the airports in descending order for on time departure percentage. Let’s start with classifying each flight as “on time” or “delayed” by creating a new variable with the mutate function. nycflights &lt;- nycflights %&gt;% mutate(dep_type = ifelse(dep_delay &lt; 5, &quot;on time&quot;, &quot;delayed&quot;)) The first argument in the mutate function is the name of the new variable we want to create, in this case dep_type. Then if dep_delay &lt; 5 we classify the flight as &quot;on time&quot; and &quot;delayed&quot; if not, i.e. if the flight is delayed for 5 or more minutes. Note that we are also overwriting the nycflights data frame with the new version of this data frame that includes the new dep_type variable. We can handle all the remaining steps in one code chunk: nycflights %&gt;% group_by(origin) %&gt;% summarise(ot_dep_rate = sum(dep_type == &quot;on time&quot;) / n()) %&gt;% arrange(desc(ot_dep_rate)) ## # A tibble: 3 x 2 ## origin ot_dep_rate ## &lt;chr&gt; &lt;dbl&gt; ## 1 LGA 0.728 ## 2 JFK 0.694 ## 3 EWR 0.637 The summarise step is telling R to count up how many records of the currently found group are on time - sum(dep_type == “on time”) - and divide that result by the total number of elements in the currently found group - n() - to get a proportion, then to store the answer in a new variable called ot_dep_rate. If you were selecting an airport simply based on on time departure percentage, which NYC airport would you choose to fly out of? EWR JFK LGA # type your code for Question 7 here, and Knit nycflights &lt;- nycflights %&gt;% mutate(dep_type = ifelse(dep_delay &lt; 5, &quot;on time&quot;, &quot;delayed&quot;)) nycflights %&gt;% group_by(origin) %&gt;% summarise(ot_dep_rate = sum(dep_type == &quot;on time&quot;) / n()) %&gt;% arrange(desc(ot_dep_rate)) ## # A tibble: 3 x 2 ## origin ot_dep_rate ## &lt;chr&gt; &lt;dbl&gt; ## 1 LGA 0.728 ## 2 JFK 0.694 ## 3 EWR 0.637 We can also visualize the distribution of on on time departure rate across the three airports using a segmented bar plot. ggplot(data = nycflights, aes(x = origin, fill = dep_type)) + geom_bar() Mutate the data frame so that it includes a new variable that contains the average speed, avg_speed traveled by the plane for each flight (in mph). What is the tail number of the plane with the fastest avg_speed? Hint: Average speed can be calculated as distance divided by number of hours of travel, and note that air_time is given in minutes. If you just want to show the avg_speed and tailnum and none of the other variables, use the select function at the end of your pipe to select just these two variables with select(avg_speed, tailnum). You can Google this tail number to find out more about the aircraft. N666DN N755US N779JB N947UW N959UW # type your code for Question 8 here, and Knit nycflights &lt;- nycflights %&gt;% mutate(avg_speed = (distance)*60/air_time) #arrange(desc(avg_speed)) nycflights %&gt;% select(tailnum, avg_speed) %&gt;% arrange(desc(avg_speed)) ## # A tibble: 32,735 x 2 ## tailnum avg_speed ## &lt;chr&gt; &lt;dbl&gt; ## 1 N666DN 703. ## 2 N779JB 557. ## 3 N571JB 554. ## 4 N568JB 548. ## 5 N5EHAA 548. ## 6 N656JB 548. ## 7 N789JB 545. ## 8 N516JB 539. ## 9 N648JB 536. ## 10 N510JB 536. ## # ... with 32,725 more rows Make a scatterplot of avg_speed vs. distance. Which of the following is true about the relationship between average speed and distance. As distance increases the average speed of flights decreases. The relationship is linear. There is an overall postive association between distance and average speed. There are no outliers. The distribution of distances are uniform over 0 to 5000 miles. # type your code for Question 9 here, and Knit #nycflights %&gt;% ggplot(nycflights, aes(x = distance, y = avg_speed)) + geom_point() Suppose you define a flight to be “on time” if it gets to the destination on time or earlier than expected, regardless of any departure delays. Mutate the data frame to create a new variable called arr_type with levels &quot;on time&quot; and &quot;delayed&quot; based on this definition. Then, determine the on time arrival percentage based on whether the flight departed on time or not. What proportion of flights that were &quot;delayed&quot; departing arrive &quot;on time&quot;? [NUMERIC INPUT] # type your code for Question 10 here, and Knit nycflights &lt;- nycflights %&gt;% mutate(dep_type = ifelse(dep_delay &lt; 5, &quot;on time&quot;, &quot;delayed&quot;)) nycflights &lt;- nycflights %&gt;% mutate(arr_type = ifelse(arr_delay &gt; 0, &quot;delayed&quot;, &quot;on time&quot;)) nycflights %&gt;% group_by(dep_type) %&gt;% summarise(ot_del_arr = sum(arr_type==&quot;on time&quot;)/n(),sum(arr_type==&quot;on time&quot;),n()) ## # A tibble: 2 x 4 ## dep_type ot_del_arr `sum(arr_type == &quot;on time&quot;)` `n()` ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 delayed 0.183 1898 10351 ## 2 on time 0.776 17375 22384 "],
["week-3-probability.html", "Week 3 - Probability 0.8 Hot Hands 0.9 Getting Started 0.10 Compared to What? 0.11 Simulations in R 0.12 Simulating the Independent Shooter", " Week 3 - Probability Complete all Exercises, and submit answers to Questions on the Coursera platform. 0.8 Hot Hands Basketball players who make several baskets in succession are described as having a hot hand. Fans and players have long believed in the hot hand phenomenon, which refutes the assumption that each shot is independent of the next. However, a 1985 paper by Gilovich, Vallone, and Tversky collected evidence that contradicted this belief and showed that successive shots are independent events. This paper started a great controversy that continues to this day, as you can see by Googling hot hand basketball. We do not expect to resolve this controversy today. However, in this lab we’ll apply one approach to answering questions like this. The goals for this lab are to (1) think about the effects of independent and dependent events, (2) learn how to simulate shooting streaks in R, and (3) to compare a simulation to actual data in order to determine if the hot hand phenomenon appears to be real. 0.9 Getting Started 0.9.1 Load packages In this lab we will explore the data using the dplyr package and visualize it using the ggplot2 package for data visualization. The data can be found in the companion package for this course, statsr. Let’s load the packages. library(statsr) library(dplyr) library(ggplot2) 0.9.2 Data Our investigation will focus on the performance of one player: Kobe Bryant of the Los Angeles Lakers. His performance against the Orlando Magic in the 2009 NBA finals earned him the title Most Valuable Player and many spectators commented on how he appeared to show a hot hand. Let’s load some necessary files that we will need for this lab. data(kobe_basket) This data frame contains 133 observations and 6 variables, where every row records a shot taken by Kobe Bryant. The shot variable in this dataset indicates whether the shot was a hit (H) or a miss (M). Just looking at the string of hits and misses, it can be difficult to gauge whether or not it seems like Kobe was shooting with a hot hand. One way we can approach this is by considering the belief that hot hand shooters tend to go on shooting streaks. For this lab, we define the length of a shooting streak to be the number of consecutive baskets made until a miss occurs. For example, in Game 1 Kobe had the following sequence of hits and misses from his nine shot attempts in the first quarter: \\[ \\textrm{H M | M | H H M | M | M | M} \\] You can verify this by viewing the first 8 rows of the data in the data viewer. Within the nine shot attempts, there are six streaks, which are separated by a “|” above. Their lengths are one, zero, two, zero, zero, zero (in order of occurrence). Fill in the blank: A streak length of 1 means one ___ followed by one miss. hit miss Fill in the blank: A streak length of 0 means one ___ which must occur after a miss that ended the preceeding streak. hit miss Counting streak lengths manually for all 133 shots would get tedious, so we’ll use the custom function calc_streak to calculate them, and store the results in a data frame called kobe_streak as the length variable. kobe_streak &lt;- calc_streak(kobe_basket$shot) We can then take a look at the distribution of these streak lengths. ggplot(data = kobe_streak, aes(x = length)) + geom_histogram(binwidth = 1) Which of the following is false about the distribution of Kobe’s streak lengths from the 2009 NBA finals. The distribution of Kobe’s streaks is unimodal and right skewed. The typical length of a streak is 0 since the median of the distribution is at 0. The IQR of the distribution is 1. The longest streak of baskets is of length 4. The shortest streak is of length 1. kobe_streak %&gt;% summarise(kobe_streak,median(length), IQR(length)) ## Warning in format.data.frame(x, digits = digits, na.encode = FALSE): ## corrupt data frame: columns will be truncated or padded with NAs ## kobe_streak median(length) IQR(length) ## 1 1 0 1 0.10 Compared to What? We’ve shown that Kobe had some long shooting streaks, but are they long enough to support the belief that he had hot hands? What can we compare them to? To answer these questions, let’s return to the idea of independence. Two processes are independent if the outcome of one process doesn’t effect the outcome of the second. If each shot that a player takes is an independent process, having made or missed your first shot will not affect the probability that you will make or miss your second shot. A shooter with a hot hand will have shots that are not independent of one another. Specifically, if the shooter makes his first shot, the hot hand model says he will have a higher probability of making his second shot. Let’s suppose for a moment that the hot hand model is valid for Kobe. During his career, the percentage of time Kobe makes a basket (i.e. his shooting percentage) is about 45%, or in probability notation, \\[ P(\\textrm{shot 1 = H}) = 0.45 \\] If he makes the first shot and has a hot hand (not independent shots), then the probability that he makes his second shot would go up to, let’s say, 60%, \\[ P(\\textrm{shot 2 = H} \\, | \\, \\textrm{shot 1 = H}) = 0.60 \\] As a result of these increased probabilites, you’d expect Kobe to have longer streaks. Compare this to the skeptical perspective where Kobe does not have a hot hand, where each shot is independent of the next. If he hit his first shot, the probability that he makes the second is still 0.45. \\[ P(\\textrm{shot 2 = H} \\, | \\, \\textrm{shot 1 = H}) = 0.45 \\] In other words, making the first shot did nothing to effect the probability that he’d make his second shot. If Kobe’s shots are independent, then he’d have the same probability of hitting every shot regardless of his past shots: 45%. Now that we’ve phrased the situation in terms of independent shots, let’s return to the question: how do we tell if Kobe’s shooting streaks are long enough to indicate that he has hot hands? We can compare his streak lengths to someone without hot hands: an independent shooter. 0.11 Simulations in R While we don’t have any data from a shooter we know to have independent shots, that sort of data is very easy to simulate in R. In a simulation, you set the ground rules of a random process and then the computer uses random numbers to generate an outcome that adheres to those rules. As a simple example, you can simulate flipping a fair coin with the following. coin_outcomes &lt;- c(&quot;heads&quot;, &quot;tails&quot;) sample(coin_outcomes, size = 1, replace = TRUE) ## [1] &quot;tails&quot; The vector outcomes can be thought of as a hat with two slips of paper in it: one slip says heads and the other says tails. The function sample draws one slip from the hat and tells us if it was a head or a tail. Run the second command listed above several times. Just like when flipping a coin, sometimes you’ll get a heads, sometimes you’ll get a tails, but in the long run, you’d expect to get roughly equal numbers of each. If you wanted to simulate flipping a fair coin 100 times, you could either run the function 100 times or, more simply, adjust the size argument, which governs how many samples to draw (the replace = TRUE argument indicates we put the slip of paper back in the hat before drawing again). Save the resulting vector of heads and tails in a new object called sim_fair_coin. sim_fair_coin &lt;- sample(coin_outcomes, size = 100, replace = TRUE) To view the results of this simulation, type the name of the object and then use table to count up the number of heads and tails. sim_fair_coin ## [1] &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; ## [9] &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; ## [17] &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; ## [25] &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; ## [33] &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; ## [41] &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; ## [49] &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; ## [57] &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; ## [65] &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; ## [73] &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; ## [81] &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; ## [89] &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; ## [97] &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; table(sim_fair_coin) ## sim_fair_coin ## heads tails ## 47 53 Since there are only two elements in outcomes, the probability that we “flip” a coin and it lands heads is 0.5. Say we’re trying to simulate an unfair coin that we know only lands heads 20% of the time. We can adjust for this by adding an argument called prob, which provides a vector of two probability weights. sim_unfair_coin &lt;- sample(coin_outcomes, size = 100, replace = TRUE, prob = c(0.2, 0.8)) table(sim_unfair_coin) ## sim_unfair_coin ## heads tails ## 19 81 prob = c(0.2, 0.8) indicates that for the two elements in the outcomes vector, we want to select the first one, heads, with probability 0.2 and the second one, tails with probability 0.8. Another way of thinking about this is to think of the outcome space as a bag of 10 chips, where 2 chips are labeled “head” and 8 chips “tail”. Therefore at each draw, the probability of drawing a chip that says “head”&quot; is 20%, and “tail” is 80%. Exercise: In your simulation of flipping the unfair coin 100 times, how many flips came up heads? In a sense, we’ve shrunken the size of the slip of paper that says “heads”, making it less likely to be drawn and we’ve increased the size of the slip of paper saying “tails”, making it more likely to be drawn. When we simulated the fair coin, both slips of paper were the same size. This happens by default if you don’t provide a prob argument; all elements in the outcomes vector have an equal probability of being drawn. If you want to learn more about sample or any other function, recall that you can always check out its help file with ?sample. 0.12 Simulating the Independent Shooter Simulating a basketball player who has independent shots uses the same mechanism that we use to simulate a coin flip. To simulate a single shot from an independent shooter with a shooting percentage of 50% we type, shot_outcomes &lt;- c(&quot;H&quot;, &quot;M&quot;) sim_basket &lt;- sample(shot_outcomes, size = 1, replace = TRUE) To make a valid comparison between Kobe and our simulated independent shooter, we need to align both their shooting percentage and the number of attempted shots. Exercise: What change needs to be made to the sample function so that it reflects a shooting percentage of 45%? Make this adjustment, then run a simulation to sample 133 shots. Assign the output of this simulation to a new object called sim_basket. # type your code for the Exercise here, and Knit sim_basket &lt;- sample(shot_outcomes, size = 133, replace = TRUE, prob = c(0.45,0.55)) Note that we’ve named the new vector sim_basket, the same name that we gave to the previous vector reflecting a shooting percentage of 50%. In this situation, R overwrites the old object with the new one, so always make sure that you don’t need the information in an old vector before reassigning its name. With the results of the simulation saved as sim_basket, we have the data necessary to compare Kobe to our independent shooter. Both data sets represent the results of 133 shot attempts, each with the same shooting percentage of 45%. We know that our simulated data is from a shooter that has independent shots. That is, we know the simulated shooter does not have a hot hand. 0.12.1 Comparing Kobe Bryant to the Independent Shooter Exercise: Using calc_streak, compute the streak lengths of sim_basket, and save the results in a data frame called sim_streak. Note that since the sim_streak object is just a vector and not a variable in a data frame, we don’t need to first select it from a data frame like we did earlier when we calculated the streak lengths for Kobe’s shots. # type your code for the Exercise here, and Knit sim_streak &lt;- calc_streak(sim_basket) Exercise: Make a plot of the distribution of simulated streak lengths of the independent shooter. What is the typical streak length for this simulated independent shooter with a 45% shooting percentage? How long is the player’s longest streak of baskets in 133 shots? # type your code for the Exercise here, and Knit ggplot(data = sim_streak, aes(x = length)) +geom_histogram(binwidth = 1) max(sim_streak) If you were to run the simulation of the independent shooter a second time, how would you expect its streak distribution to compare to the distribution from the exercise above? Exactly the same Somewhat similar Totally different #independent shooter sim 2 sim_basket1 &lt;- sample(shot_outcomes, size = 133, replace = TRUE, prob = c(0.45,0.55)) sim_streak1 &lt;- calc_streak(sim_basket1) ggplot(sim_streak1, aes(x = length)) + geom_histogram(binwidth = 1) How does Kobe Bryant’s distribution of streak lengths compare to the distribution of streak lengths for the simulated shooter? Using this comparison, do you have evidence that the hot hand model fits Kobe’s shooting patterns? The distributions look very similar. Therefore, there doesn’t appear to be evidence for Kobe Bryant’s hot hand. The distributions look very similar. Therefore, there appears to be evidence for Kobe Bryant’s hot hand. The distributions look very different. Therefore, there doesn’t appear to be evidence for Kobe Bryant’s hot hand. The distributions look very different. Therefore, there appears to be evidence for Kobe Bryant’s hot hand. Exercise: What concepts from the course videos are covered in this lab? What concepts, if any, are not covered in the videos? Have you seen these concepts elsewhere, e.g. textbook, previous labs, or practice problems? This is a derivative of an OpenIntro lab, and is released under a Attribution-NonCommercial-ShareAlike 3.0 United States license. "],
["week-4-project-exploring-the-brfss-data-by-akshay-kotha.html", "Week 4 - Project - Exploring the BRFSS data by Akshay Kotha 0.13 Setup 0.14 Part 1: Data 0.15 Part 2: Research questions 0.16 Part 3: Exploratory data analysis", " Week 4 - Project - Exploring the BRFSS data by Akshay Kotha Refer /brfss_codebook.html for details on the BRFSS variables. 0.13 Setup 0.13.1 Load packages library(ggplot2) library(dplyr) library(statsr) 0.13.2 Load data load(&quot;brfss2013.RData&quot;) 0.14 Part 1: Data There are two types of observations in general. 1. Data collected via landline telephone interviews. 2. Data collected via cellular phone interviews. It is also mentioned that all the responses were self-reported which is similar to volunteering to answer the questionnairre. When it comes to landline telephone interviews, disproportionte stratified sampling(DSS) was done which implies the results would be representative of entire population. DSS might have been done to cater for the need to represent the entire population. When it comes to cellular phone interviews, it is mentioned that random sampling took place. Based on this, the results and analysis obtained can be generalized to the US population or a population with similar characteristics because the random samples are representative of the entire population across all the states of the US. In both cases, causality cannot be inferred as this is only an observational study which has non-response bias and nowhere it was mentioned that random assignment was done. 0.15 Part 2: Research questions 0.15.1 Research quesion 1: Relation between height(htin4), weight (wtkg3) and ‘joinpain’ (how bad was joint pain?)Describe the distributions and which probabilistic distributions are skewed (positively or negatively/ right or left) - Distribution of Height(inches) htin4 w.r.t ‘joinpain’ or wtkg3 w.r.t ‘joinpain’? This is of interest because it helps understand to decide on which variable to use for predictive modelling. If the variables are highly skewed, they have to be transformed and then used to get accurae predictions. 0.15.2 Research quesion 2: Relation between people who have coronary heart disease (cvdcrhd4) and those who are diagnosed with heartattack (cvfinfr4) using comparison between states of maximum adnd minimum respondents? Association of two same organ ailments would be helpful in whether both have to be treated separately or together. The check of whether this varies across different states is to understand whether it matters if the people are located in one state over the other. It can be understood whether ’_state’ variable has any association. For instance if it really varies between different states, more variables can be thought about from within the data or externally during the causal analysis. 0.15.3 Research quesion 3: Are frequency of feeling depressed in the past days (misdeprd), difficulty in concentrating or remembering (decide) are associated or dependent? Correlation finding is useful as this is an observational study and eventually might help in finding stronger evidence for causality (only after carrying some random experiments but not from this study solely). 0.16 Part 3: Exploratory data analysis 0.16.1 Research quesion 1: Code #checking the type of variable str(brfss2013$wtkg3) ## int [1:491775] 11340 5761 7257 5806 12020 10206 4808 NA 10659 7711 ... str(brfss2013$htin4) ## int [1:491775] 67 70 64 64 72 63 60 65 74 65 ... str(brfss2013$joinpain) ## int [1:491775] 7 NA 5 NA NA NA 3 8 4 NA ... #creating new df so that there are no &#39;NA&#39; values in the varibles under consideration brfss_joinpain &lt;- brfss2013 %&gt;% filter(!is.na(joinpain),!is.na(htin4), !is.na(wtkg3)) %&gt;% mutate(wtkg3_actual = wtkg3/100) ##Assumption: The calculated weight variable wtkg3 divided by 100 makes sense hence added new variable wtkg3_actual #str(brfss_joinpain$wtkg3_actual) #converting int values of levels in &#39;joinpain&#39; to factor so that they can be ordered properly in denotion brfss_joinpain[, &#39;joinpain&#39;] &lt;- factor(brfss_joinpain[,&#39;joinpain&#39;]) str(brfss_joinpain$joinpain) #%&gt;% ## Factor w/ 11 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 8 6 4 5 9 6 5 3 8 8 ... #TO get an idea of summary statistics of heights brfss_joinpain %&gt;% group_by(joinpain) %&gt;% summarise(count=n(), mean_height = mean(htin4), median_ht = median(htin4), min_ht = min(htin4), max_ht = max(htin4), iqr_ht = IQR(htin4), sd_ht = sd(htin4), var_ht = var(htin4)) ## # A tibble: 11 x 9 ## joinpain count mean_height median_ht min_ht max_ht iqr_ht sd_ht var_ht ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 10658 66.4 66 40 87 7 4.18 17.5 ## 2 1 8716 66.7 66 48 83 6 4.09 16.7 ## 3 2 14330 66.7 66 2 81 6 4.08 16.6 ## 4 3 17800 66.5 66 36 92 6 4.03 16.3 ## 5 4 15440 66.2 66 48 83 6 4.03 16.2 ## 6 5 22675 65.8 65 41 87 5 4.00 16.0 ## 7 6 12169 66.0 66 45 85 6 4.10 16.8 ## 8 7 13073 66.4 66 48 6123 6 53.1 2823. ## 9 8 14739 65.6 65 38 86 5 4.08 16.7 ## 10 9 3728 65.5 65 50 84 5 4.07 16.6 ## 11 10 8252 65.0 64 48 90 5 3.95 15.6 #To get an idea of summary stats of weights (kg) brfss_joinpain %&gt;% group_by(joinpain) %&gt;% summarise(count=n(), mean_wt = mean(wtkg3_actual), median_wt = median(wtkg3_actual), min_wt = min(wtkg3_actual), max_wt = max(wtkg3_actual), iqr_wt = IQR(wtkg3_actual), sd_wt = sd(wtkg3_actual), var_wt = var(wtkg3_actual)) ## # A tibble: 11 x 9 ## joinpain count mean_wt median_wt min_wt max_wt iqr_wt sd_wt var_wt ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 10658 79.7 77.1 25.0 231. 25.4 20.3 410. ## 2 1 8716 79.4 77.1 36.3 209. 25.0 19.2 368. ## 3 2 14330 81.1 79.4 0.02 213. 22.7 19.4 378. ## 4 3 17800 81.7 79.4 31.8 290. 24.5 20.1 403. ## 5 4 15440 82.0 79.4 25.0 250 24.9 21.0 441. ## 6 5 22675 81.3 78.5 22.7 272. 24.0 20.9 435. ## 7 6 12169 83.9 81.6 24.5 261. 27.2 22.6 511. ## 8 7 13073 85.3 81.6 0.02 272. 29.5 23.1 532. ## 9 8 14739 84.8 81.6 24.5 263. 29.5 23.7 562. ## 10 9 3728 86.8 82.6 36.3 207. 29.9 24.6 604. ## 11 10 8252 84.4 81.2 24.0 272. 29.0 24.0 578. #plot probability distributions by categorical level of the variable &#39;joinpain&#39; for height #dim(brfss_joinpain) ggplot(brfss_joinpain, aes(x = htin4, colour = joinpain)) + geom_density() + labs(x = &quot;height (inches)&quot;, y = &quot;prob_density&quot;) #plot probability distributions by categorical level of the variable &#39;joinpain&#39; for weight ggplot(brfss_joinpain, aes(x = wtkg3_actual, colour = joinpain)) + geom_density() + labs(x = &quot;weight (kg)&quot;, y = &quot;prob_density&quot;) 0.16.2 Narrative of question 1: With respect to own scales of the above two plots, distributions of heigts and weights across categories of the joinpain variable are both right skewed. Skewness of both the plots can be verified with the summary statistics calculated before the plots were made. Both of them need further adjustment via normalization(not understood completely, out of scope for this course) or other techniques to use them in the predictive models for higher accuracy. 0.16.3 Research quesion 2: Code #check variable type str(brfss2013$cvdinfr4) ## Factor w/ 2 levels &quot;Yes&quot;,&quot;No&quot;: 2 2 2 2 2 2 2 2 2 2 ... str(brfss2013$cvdcrhd4) ## Factor w/ 2 levels &quot;Yes&quot;,&quot;No&quot;: NA 2 2 2 2 2 2 1 2 2 ... str(brfss2013$X_state) ## Factor w/ 55 levels &quot;0&quot;,&quot;Alabama&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... #_state is not allowed, so replacement is done grep(&quot;state&quot;, names(brfss2013), value = TRUE) ## [1] &quot;X_state&quot; &quot;stateres&quot; &quot;cstate&quot; # number of respondents grouped by state statewise_count &lt;- brfss2013 %&gt;% group_by(X_state) %&gt;% summarise(count = n()) #arrange(desc(countX_state)) #finding the descending order statewise_count %&gt;% #summarise(max(count), min(count)) arrange(desc(count)) ## # A tibble: 55 x 2 ## X_state count ## &lt;fct&gt; &lt;int&gt; ## 1 Florida 33668 ## 2 Kansas 23282 ## 3 Nebraska 17139 ## 4 Massachusetts 15071 ## 5 Minnesota 14340 ## 6 New Jersey 13776 ## 7 Colorado 13649 ## 8 Maryland 13011 ## 9 Utah 12769 ## 10 Michigan 12761 ## # ... with 45 more rows # new dataframe creation for the purpose of answering this specific question brfss_heartattack &lt;- brfss2013 %&gt;% filter(!is.na(cvdinfr4),!is.na(cvdcrhd4), X_state %in% c(&quot;Florida&quot;,&quot;Guam&quot;)) #New variable to get percentage in the plots based on select variable brfss_heartattack %&gt;% group_by(X_state, cvdinfr4, cvdcrhd4) %&gt;% summarise(count = n()) %&gt;% mutate(percentage_count = 100 * count/sum(count)) %&gt;% #plot coronary heart disease along x axis, cvdinfr4 in a different colour ggplot(aes(x=cvdcrhd4, y = percentage_count, fill=cvdinfr4)) + #plot cvdinfr4 alongside cvdcrhd4 (&quot;dodge&quot;, alternatively use &quot;stack&quot;) and make #a seperate graph for both values of cvdcrhd4 geom_bar(stat = &quot;identity&quot;,position = &quot;dodge&quot;) + # To split across the states considered facet_wrap(~X_state) + # Beautify with color codes scale_fill_manual(&quot;Condition&quot;, values = alpha( c(&quot;firebrick&quot;, &quot;dodgerblue4&quot;), 1) ) + labs(x = &quot;Ever Diagnosed With Angina Or Coronary Heart Disease&quot;, y = &quot;percentage diagnosed with heart attack based on x-variable&quot;) 0.16.4 Narrative of question 2: The X_state variable is not associated with the link between diagnosing with heart attack based on having coronory heart disease because no matter the state has maximum respondents (Florida) or minimum respondents (Guam), the proportions of those who are diagnosed with coronory heart disease are also diagnosed with heart attack are almost same. Implicity, there is an association between having coronoary heart disease and not having coronoary heart disease with having a heart attack. Hence, variable x is associated with the heart attack condition but state variable is not associated with the correlation of earlier two. Summary statistics answering this question is out of scope for this question because it is an analysis between categorical variables. 0.16.5 Research quesion 3: Code #checking variable type str(brfss2013$misdeprd) ## Factor w/ 5 levels &quot;All&quot;,&quot;Most&quot;,&quot;Some&quot;,..: 5 5 5 5 NA NA NA NA NA NA ... str(brfss2013$decide) ## Factor w/ 2 levels &quot;Yes&quot;,&quot;No&quot;: 2 2 2 2 2 2 2 2 2 2 ... #data frame without NAs for the variables under consideration brfss_dep_conc &lt;- brfss2013 %&gt;% filter(!is.na(misdeprd),!is.na(decide)) #group_by(misdeprd, decide) %&gt;% #summarise(n()) #stacked bar plot ggplot(brfss_dep_conc, aes(x = misdeprd)) + geom_bar(aes(fill = decide), position = &#39;fill&#39;) + labs(x = &quot;How Often Feel Depressed Past 30 Days&quot;, y = &quot;proportion of Yes/No for having diffculty concentrating&quot;) 0.16.6 Narrative question 3: The stacked bar plot above shows that there is a clear association (dependency) between DECIDE and MISDEPRD. There is an increasing trend for proportion of people who didn’t face difficulty in concentrating or remembering (decide) in each category of the variable on x-axis (How often people were depressed in the last 30 days) from left to right. Summary statistics answering this question is out of scope for this question because it is an analysis between categorical variables. "]
]
